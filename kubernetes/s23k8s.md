
### pv pvc common model:

- nfs : no need to do it on each worker manually. nfs provisioner make all provisioned on each worker
- file system  - need to make file system manually on each worker
- ceph





#### nfs sample step by step and yaml code:


***1- make install nfs server on master:***

apt update

apt install nfs-kernel-server  - and make config it based on best practice


***2- make install nfs common on each worker:***

apt update

apt install nfs-common


***3- create yaml file for pv:***

vi pv-nfs.yml

        
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: nfs-pv-ali
        spec:
          capacity:
             storage: 200Mi
          volumeMode: Filesystem
          accessModes:
             - ReadWriteMany
          persistentVolumeReclaimPolicy: Recycle
          storageClassName: nfs
          mountOptions:
            - hard
            - nfsvers=4.1
          nfs:
            path: /data
            server: 192.168.44.136
        
        


 kubectl apply -f pv-nfs.yml

 

make sample pod (nginx) to claim pvc:

vi nginxnfs.yml


                        apiVersion: v1
                        kind: Deployment
                        namespace: default
                        metadata:
                          name: nginx-deploy
                          namespace: default
                          labels:
                            app: nginx
                        spec:
                          replicas: 2
                          selector:
                             matchLabels:
                                app: nginx
                          template:
                             metadata:
                                 labels:
                                    app: nginx
                             spec:
                                containers:
                                     - name: nginx-ctr
                                       image: nginx:latest
                                       ports:
                                          - containerPort: 80
                                       volumeMounts:
                                          - mountsPath: "/usr/share/nginx/html"
                                            name: nginx-pv-storage
                          volumes:
                            - name: nginx-pv-stroage
                              persistentVolumeClaim:
                                  claimName: nfs-pvc
                        






in cloud network that include pool storage you can use ceph instead of nfs. its provisioner too.

to do senario in production first test it in your test environment. that how your config works.


cloud native computing foundation site:


cncf.io


refrence for uptodate devops tool is:


landscape.cncf.io


for example in container registery: nowday harbor is famous than nexus.


cncf graduated means that have cncf license and supervision it.


also site:

roadmap.sh/devops




to apply multiple resource we have 2 options:

1- write all resource in 1 yaml file and seperate them with 3dash role (---) 

2- write each resource separate yaml file store in one directory and apply -f thatdirectory:

kubectl apply -f relatedyamlresourcedirectory/  - apply all in once



#### create ns

1- kubectl create namespace test

2- in yaml file with kind Namespace and name and label in metadata

3- in apply command: kubectl apply -f tets.yaml -n test   - make ns test if not available

4- in kubens   - same as above



kubectl get all  - show all resource

kubectl delete all --all   - delete everything make your k8s clear. its for your test environments


# resource request and limit

request= reserve. in principle is pod base but bestpractice is that we can define resource limit for namespace to aviod forgetiing do it for each container . for  memory and cpu . in spec section of container definitions. its apply by containerruntime. 

note that cpu unit in k8s is milicore. 500m=0.5core. 1000m = 1 core

                        resources:
                           limits:
                               memory: "200Mi"
                               cpu: "200m"
                           requests:
                               memory: "100Mi"
                               cpu: "1000m"






before make desicion to choose write plan or senario for cluster you should do t on test environmet and load test software and send virtual terafic on them then if one resource use 75% you should expand that resource. this test reaulary design and do by QA or test team. never leave your resource (docker- copose swarm - k8s,..) without limit its necessary to assign limit to all your resource to prevent lack of your resource in abnormal posiotion. for ex.: ddos attack , abnrnaml function of app, or bad developed software ,..... .  to prevent affect server resource. our server should not affect never. load testing tools are like: k6s (ex.:define time increase load in 30 interval)


resources return multiple actions again suddenly increase load or step by step increasing load. may crash on suddenly increase (for ex.: webinar in specifed time all user login)








# resource quota:

define resource quota like below (for ex. for harware) and then called in pod or deployment or namespace. 



apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo
spec:
  hard:
    requests.cpu: 500m
    requests.memory: 100Mib
    limits.cpu: 700m
    limits.memory: 500Mib




quota description:

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/


kubectl create namespace quota-pod-example


apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
spec:
  hard:
    pods: "2"




kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example



apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-quota-demo
spec:
  selector:
    matchLabels:
      purpose: quota-demo
  replicas: 3
  template:
    metadata:
      labels:
        purpose: quota-demo
    spec:
      containers:
      - name: pod-quota-demo
        image: nginx
    

kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example

kubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml



The output shows that even though the Deployment specifies three replicas, only two Pods were created because of the quota you defined earlier:

spec:
  ...
  replicas: 3
...
status:
  availableReplicas: 2
...
lastUpdateTime: 2021-04-02T20:57:05Z
    message: 'unable to create pods: pods "pod-quota-demo-1650323038-" is forbidden:
      exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited: pods=2'




you can assign namespace for resource quota in apply commadn or  in resource quota definition yaml file. after that all resource that bond to that namespace have resource quota.




kubectl delete namespace quota-pod-example





# limitRange

its another limit resource in k8s that defined with in kind: LimitRange

By default, containers run with unbounded compute resources on a Kubernetes cluster. Using Kubernetes resource quotas, administrators (also termed cluster operators) can restrict consumption and creation of cluster resources (such as CPU time, memory, and persistent storage) within a specified namespace. Within a namespace, a Pod can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace.

A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace.

A LimitRange provides constraints that can:

 - Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.
 - Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.
 - Enforce a ratio between request and limit for a resource in a namespace.
 - Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.


A LimitRange is enforced in a particular namespace when there is a LimitRange object in that namespace.

The name of a LimitRange object must be a valid DNS subdomain name.



                        apiVersion: v1
                        kind: LimitRange
                        metadata:
                          name: mem-limit-range
                        spec:
                          limits:
                          - default:
                              memory: 512Mi
                            defaultRequest:
                              memory: 256Mi
                            type: Container
                        




# cron job

similar to linux. schedule a job as a container. dont write restart policy : always becouse it restarted every interval 1sec or 2sec even after exit(0). let it be restartPolicy: onFailure . A CronJob creates Jobs on a repeating schedule.

CronJob is meant for performing regular scheduled actions such as backups, report generation, and so on. also you be able to set timezone for your cron job other than aerver or laptop time zone.

cron job api version is batch/v1 to see k8s resources api versions run:

kubectl get apiservices.apiregistration.k8s.io


 vi cronjob.yaml



                        apiVersion: batch/v1
                        kind: CronJob
                        metadata:
                          name: hello
                        spec:
                          schedule: "*/1 * * * *"
                          jobTemplate:
                            spec:
                              template:
                                spec:
                                  containers:
                                  - name: hello
                                    image: busybox
                                    args:
                                    - /bin/sh
                                    - -c
                                    - date; echo Hello from the Kubernetes cluster
                                  restartPolicy: OnFailure
                                  
          
          
kubectl apply -f cronjob.yaml


kubectl get jobs.batch hello-28223640


 kubectl get jobs.batch




 # config map

 is a type of volume. 




 To store a configuration file made of key value pairs, or simply to store a generic file you can use a so-called config map and mount it inside a Pod:


kubectl create configmap velocity --from-file=index.html



The mount looks like this:
...
spec:
  containers:
  - image: busybox
...
    volumeMounts:
    - mountPath: /velocity
      name: test
    name: busybox
  volumes:
  - name: test
    configMap:
      name: velocity




volume type:

1- mounthpath - host path - empty directory   - dorectory sharing

2- pv (local -nfs - ceph) - pvc   - directory sharing

3- config map: just for file (config file) not sharing  dorectory


for ex. : we wnat to run gitlab as pod then we prepare config file for gitlab called gitlab.rb then after gitlab pod running copy this file to pod.


we can specified config ap with command or in yaml file. even we can tell to config map to write content in that file write from pod yaml file in with | command like below:

                        ---
                        # https://kubernetes.io/docs/concepts/storage/volumes/#configmap
                        apiVersion: v1
                        kind: Pod
                        metadata:
                          name: volumes-configmap-pod
                        spec:
                          containers:
                            - command:
                                - sleep
                                - "3600"
                              image: busybox
                              name: volumes-configmap-pod-container
                              volumeMounts:
                                - name: volumes-configmap-volume
                                  mountPath: /etc/config
                          volumes:
                            - name: volumes-configmap-volume
                              configMap:
                                name: volumes-configmap-configmap
                                items:
                                  - key: game.properties
                                    path: configmap-volume-path
                        ---
                        apiVersion: v1
                        kind: ConfigMap
                        metadata:
                          name: volumes-configmap-configmap
                        data:
                          game.properties: |
                            enemies=aliens
                            lives=3
                            enemies.cheat=true
                            enemies.cheat.level=noGoodRotten
                          ui.properties: |
                            color.good=purple
                            color.bad=yellow
                        
                            



kubectl apply -f configmap.yml

kubectl get configmaps


to see config map:

 kubectl exec -it volumes-configmap-pod -- sh

 cd /etc/config

 ls -l

                        lrwxrwxrwx    1 root     root            28 Aug 30 18:15 configmap-volume-path -> ..data/configmap-volume-path


 cat configmap-volume-path

                        
                        enemies=aliens
                        lives=3
                        enemies.cheat=true
                        enemies.cheat.level=noGoodRotten
                        


***hint: one another way ot get yaml file of one resource is below command:


kubectl edit pods nginxpod


# init-containers






