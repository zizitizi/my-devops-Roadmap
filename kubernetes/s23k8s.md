
### pv pvc common model:

- nfs : no need to do it on each worker manually. nfs provisioner make all provisioned on each worker
- file system  - need to make file system manually on each worker
- ceph





#### nfs sample step by step and yaml code:


***1- make install nfs server on master:***

apt update

apt install nfs-kernel-server  - and make config it based on best practice


***2- make install nfs common on each worker:***

apt update

apt install nfs-common


***3- create yaml file for pv:***

vi pv-nfs.yml

        
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: nfs-pv-ali
        spec:
          capacity:
             storage: 200Mi
          volumeMode: Filesystem
          accessModes:
             - ReadWriteMany
          persistentVolumeReclaimPolicy: Recycle
          storageClassName: nfs
          mountOptions:
            - hard
            - nfsvers=4.1
          nfs:
            path: /data
            server: 192.168.44.136
        
        


 kubectl apply -f pv-nfs.yml

 

make sample pod (nginx) to claim pvc:

vi nginxnfs.yml


                        apiVersion: v1
                        kind: Deployment
                        namespace: default
                        metadata:
                          name: nginx-deploy
                          namespace: default
                          labels:
                            app: nginx
                        spec:
                          replicas: 2
                          selector:
                             matchLabels:
                                app: nginx
                          template:
                             metadata:
                                 labels:
                                    app: nginx
                             spec:
                                containers:
                                     - name: nginx-ctr
                                       image: nginx:latest
                                       ports:
                                          - containerPort: 80
                                       volumeMounts:
                                          - mountsPath: "/usr/share/nginx/html"
                                            name: nginx-pv-storage
                          volumes:
                            - name: nginx-pv-stroage
                              persistentVolumeClaim:
                                  claimName: nfs-pvc
                        






in cloud network that include pool storage you can use ceph instead of nfs. its provisioner too.

to do senario in production first test it in your test environment. that how your config works.


cloud native computing foundation site:


cncf.io


refrence for uptodate devops tool is:


landscape.cncf.io


for example in container registery: nowday harbor is famous than nexus.


cncf graduated means that have cncf license and supervision it.


also site:

roadmap.sh/devops




to apply multiple resource we have 2 options:

1- write all resource in 1 yaml file and seperate them with 3dash role (---) 

2- write each resource separate yaml file store in one directory and apply -f thatdirectory:

kubectl apply -f relatedyamlresourcedirectory/  - apply all in once



#### create ns

1- kubectl create namespace test

2- in yaml file with kind Namespace and name and label in metadata

3- in apply command: kubectl apply -f tets.yaml -n test   - make ns test if not available

4- in kubens   - same as above



kubectl get all  - show all resource

kubectl delete all --all   - delete everything make your k8s clear. its for your test environments


# resource request and limit

request= reserve. in principle is pod base but bestpractice is that we can define resource limit for namespace to aviod forgetiing do it for each container . for  memory and cpu . in spec section of container definitions. its apply by containerruntime. 

note that cpu unit in k8s is milicore. 500m=0.5core. 1000m = 1 core

                        resources:
                           limits:
                               memory: "200Mi"
                               cpu: "200m"
                           requests:
                               memory: "100Mi"
                               cpu: "1000m"






before make desicion to choose write plan or senario for cluster you should do t on test environmet and load test software and send virtual terafic on them then if one resource use 75% you should expand that resource. this test reaulary design and do by QA or test team. never leave your resource (docker- copose swarm - k8s,..) without limit its necessary to assign limit to all your resource to prevent lack of your resource in abnormal posiotion. for ex.: ddos attack , abnrnaml function of app, or bad developed software ,..... .  to prevent affect server resource. our server should not affect never. load testing tools are like: k6s (ex.:define time increase load in 30 interval)


resources return multiple actions again suddenly increase load or step by step increasing load. may crash on suddenly increase (for ex.: webinar in specifed time all user login)








# resource quota:

define resource quota like below (for ex. for harware) and then called in pod or deployment or namespace. 



apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo
spec:
  hard:
    requests.cpu: 500m
    requests.memory: 100Mib
    limits.cpu: 700m
    limits.memory: 500Mib




quota description:

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/


kubectl create namespace quota-pod-example


apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
spec:
  hard:
    pods: "2"




kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example



apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-quota-demo
spec:
  selector:
    matchLabels:
      purpose: quota-demo
  replicas: 3
  template:
    metadata:
      labels:
        purpose: quota-demo
    spec:
      containers:
      - name: pod-quota-demo
        image: nginx
    

kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example

kubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml



The output shows that even though the Deployment specifies three replicas, only two Pods were created because of the quota you defined earlier:

spec:
  ...
  replicas: 3
...
status:
  availableReplicas: 2
...
lastUpdateTime: 2021-04-02T20:57:05Z
    message: 'unable to create pods: pods "pod-quota-demo-1650323038-" is forbidden:
      exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited: pods=2'




you can assign namespace for resource quota in apply commadn or  in resource quota definition yaml file. after that all resource that bond to that namespace have resource quota.




kubectl delete namespace quota-pod-example





# limitRange

its another resourcein k8s that defined with in kind: LimitRange











